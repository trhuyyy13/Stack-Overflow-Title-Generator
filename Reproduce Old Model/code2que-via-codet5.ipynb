{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:49:56.092742Z",
     "iopub.status.busy": "2025-05-05T09:49:56.092488Z",
     "iopub.status.idle": "2025-05-05T09:51:33.357203Z",
     "shell.execute_reply": "2025-05-05T09:51:33.356040Z",
     "shell.execute_reply.started": "2025-05-05T09:49:56.092724Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 09:50:11.497809: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746438611.682923      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746438611.740254      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® C√†i l·∫°i ƒë√∫ng phi√™n b·∫£n transformers & peft...\n",
      "Found existing installation: transformers 4.51.1\n",
      "Uninstalling transformers-4.51.1:\n",
      "  Successfully uninstalled transformers-4.51.1\n",
      "Found existing installation: peft 0.14.0\n",
      "Uninstalling peft-0.14.0:\n",
      "  Successfully uninstalled peft-0.14.0\n",
      "Collecting transformers==4.41.1\n",
      "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft==0.10.0\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.1)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (2.5.1+cu124)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (1.3.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.1) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.1) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.1) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.1) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.1) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.1) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.1) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.1) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft==0.10.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft==0.10.0)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft==0.10.0)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft==0.10.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft==0.10.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft==0.10.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft==0.10.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.10.0) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.1) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.41.1) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.41.1) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.41.1) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.41.1) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.41.1) (2024.2.0)\n",
      "Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m156.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m324.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m351.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m290.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m318.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m315.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m334.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m314.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m314.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m338.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, peft\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.10.0 tokenizers-0.19.1 transformers-4.41.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import peft\n",
    "import os\n",
    "\n",
    "\n",
    "if transformers.__version__ != \"4.41.1\" or peft.__version__ != \"0.10.0\":\n",
    "    print(\"üö® C√†i l·∫°i ƒë√∫ng phi√™n b·∫£n transformers & peft...\")\n",
    "    !pip uninstall -y transformers peft\n",
    "    !rm -rf /usr/local/lib/python3.11/dist-packages/transformers*\n",
    "    !rm -rf /usr/local/lib/python3.11/dist-packages/peft*\n",
    "    !pip install transformers==4.41.1 peft==0.10.0 --upgrade --no-cache-dir\n",
    "    #import os\n",
    "    #os._exit(0)\n",
    "else:\n",
    "    print(\"‚úÖ ƒê√∫ng phi√™n b·∫£n, ti·∫øp t·ª•c ch·∫°y.\")\n",
    "    #import os\n",
    "    #os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:51:50.339592Z",
     "iopub.status.busy": "2025-05-05T09:51:50.339303Z",
     "iopub.status.idle": "2025-05-05T09:51:53.421573Z",
     "shell.execute_reply": "2025-05-05T09:51:53.420770Z",
     "shell.execute_reply.started": "2025-05-05T09:51:50.339569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#os._exit(0)\n",
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:51:53.423329Z",
     "iopub.status.busy": "2025-05-05T09:51:53.423029Z",
     "iopub.status.idle": "2025-05-05T09:52:05.298612Z",
     "shell.execute_reply": "2025-05-05T09:52:05.297752Z",
     "shell.execute_reply.started": "2025-05-05T09:51:53.423307Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checking library versions...\n",
      "\n",
      "transformers: 4.41.1 ‚úÖ (>= 4.41.0 recommended)\n",
      "torch       : 2.5.1+cu124 ‚úÖ (>= 2.0 recommended)\n",
      "peft        : 0.10.0 ‚úÖ (>= 0.10.0 required for Trainer integration)\n",
      "datasets    : 3.5.0 ‚úÖ (>= 2.14.0 recommended)\n",
      "accelerate  : 1.3.0 ‚úÖ (>= 0.23.0 recommended)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 09:51:58.250945: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746438718.273310     101 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746438718.280132     101 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ TrainingArguments & Trainer are working correctly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6ac5bb75504014930b85b571a3a094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d3ce3dc6b7443d97235884e4326f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded CodeT5 model successfully.\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers==4.41.1 --upgrade --no-cache-dir\n",
    "#!pip uninstall -y peft\n",
    "#!pip install peft==0.10.0 --quiet\n",
    "\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import peft\n",
    "import datasets\n",
    "import accelerate\n",
    "\n",
    "\n",
    "print(\"‚úÖ Checking library versions...\\n\")\n",
    "\n",
    "print(f\"transformers: {transformers.__version__} ‚úÖ (>= 4.41.0 recommended)\")\n",
    "print(f\"torch       : {torch.__version__} ‚úÖ (>= 2.0 recommended)\")\n",
    "print(f\"peft        : {peft.__version__} ‚úÖ (>= 0.10.0 required for Trainer integration)\")\n",
    "print(f\"datasets    : {datasets.__version__} ‚úÖ (>= 2.14.0 recommended)\")\n",
    "print(f\"accelerate  : {accelerate.__version__} ‚úÖ (>= 0.23.0 recommended)\")\n",
    "\n",
    "# Ki·ªÉm tra xem TrainingArguments c√≥ ho·∫°t ƒë·ªông\n",
    "try:\n",
    "    from transformers import TrainingArguments, Trainer\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"check\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=4,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10\n",
    "    )\n",
    "    print(\"\\n‚úÖ TrainingArguments & Trainer are working correctly.\")\n",
    "except Exception as e:\n",
    "    print(\"\\n‚ùå TrainingArguments setup failed:\", e)\n",
    "\n",
    "# Ki·ªÉm tra model load\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-base\")\n",
    "    print(\"‚úÖ Loaded CodeT5 model successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to load CodeT5 model:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:52:05.300541Z",
     "iopub.status.busy": "2025-05-05T09:52:05.299864Z",
     "iopub.status.idle": "2025-05-05T09:52:05.413802Z",
     "shell.execute_reply": "2025-05-05T09:52:05.412982Z",
     "shell.execute_reply.started": "2025-05-05T09:52:05.300518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/code2que-data/Code2Que-data/csharpdata/src-train.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/csharpdata/reference.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/csharpdata/hypothesis.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/csharpdata/src-test-clear.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/csharpdata/tgt-train.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/csharpdata/tgt-test-clear.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/jsdata/src-train.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/jsdata/reference.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/jsdata/hypothesis.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/jsdata/src-test-clear.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/jsdata/tgt-train.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/jsdata/tgt-test-clear.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/sqldata/src-train.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/sqldata/reference.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/sqldata/hypothesis.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/sqldata/src-test-clear.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/sqldata/tgt-train.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/sqldata/tgt-test-clear.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/pydata/src-train.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/pydata/reference.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/pydata/hypothesis.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/pydata/src-test-clear.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/pydata/tgt-train.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/pydata/tgt-test-clear.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/javadata/src-train.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/javadata/reference.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/javadata/hypothesis.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/javadata/src-test-clear.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/javadata/tgt-train.txt\n",
      "/kaggle/input/code2que-data/Code2Que-data/javadata/tgt-test-clear.txt\n",
      "/kaggle/input/code2que-via-codet5/val_split.json\n",
      "/kaggle/input/code2que-via-codet5/train_split.json\n",
      "/kaggle/input/code2que-via-codet5/__results__.html\n",
      "/kaggle/input/code2que-via-codet5/test.json\n",
      "/kaggle/input/code2que-via-codet5/__notebook__.ipynb\n",
      "/kaggle/input/code2que-via-codet5/__output__.json\n",
      "/kaggle/input/code2que-via-codet5/custom.css\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/config.json\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/merges.txt\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/trainer_state.json\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/training_args.bin\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/tokenizer.json\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/vocab.json\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/tokenizer_config.json\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/scheduler.pt\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/model.safetensors\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/special_tokens_map.json\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/optimizer.pt\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/rng_state.pth\n",
      "/kaggle/input/code2que-via-codet5/codet5-finetuned-full/checkpoint-178830/generation_config.json\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093646-fbtu5y1q/run-fbtu5y1q.wandb\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093646-fbtu5y1q/logs/debug.log\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093646-fbtu5y1q/logs/debug-internal.log\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093646-fbtu5y1q/files/output.log\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093646-fbtu5y1q/files/requirements.txt\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093646-fbtu5y1q/files/wandb-metadata.json\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093339-jgqw76p6/run-jgqw76p6.wandb\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093339-jgqw76p6/logs/debug.log\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093339-jgqw76p6/logs/debug-internal.log\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093339-jgqw76p6/files/output.log\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093339-jgqw76p6/files/requirements.txt\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_093339-jgqw76p6/files/wandb-metadata.json\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_092557-hh74vn8t/run-hh74vn8t.wandb\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_092557-hh74vn8t/logs/debug.log\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_092557-hh74vn8t/logs/debug-internal.log\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_092557-hh74vn8t/files/output.log\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_092557-hh74vn8t/files/requirements.txt\n",
      "/kaggle/input/code2que-via-codet5/wandb/run-20250503_092557-hh74vn8t/files/wandb-metadata.json\n",
      "/kaggle/input/code2que-via-codet5/__results___files/__results___16_0.png\n",
      "/kaggle/input/code2que-via-codet5/codeT5-finetuned-full-/config.json\n",
      "/kaggle/input/code2que-via-codet5/codeT5-finetuned-full-/merges.txt\n",
      "/kaggle/input/code2que-via-codet5/codeT5-finetuned-full-/training_args.bin\n",
      "/kaggle/input/code2que-via-codet5/codeT5-finetuned-full-/tokenizer.json\n",
      "/kaggle/input/code2que-via-codet5/codeT5-finetuned-full-/vocab.json\n",
      "/kaggle/input/code2que-via-codet5/codeT5-finetuned-full-/tokenizer_config.json\n",
      "/kaggle/input/code2que-via-codet5/codeT5-finetuned-full-/model.safetensors\n",
      "/kaggle/input/code2que-via-codet5/codeT5-finetuned-full-/special_tokens_map.json\n",
      "/kaggle/input/code2que-via-codet5/codeT5-finetuned-full-/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-05T09:52:05.415623Z",
     "iopub.status.busy": "2025-05-05T09:52:05.415419Z",
     "iopub.status.idle": "2025-05-05T09:52:05.420481Z",
     "shell.execute_reply": "2025-05-05T09:52:05.419895Z",
     "shell.execute_reply.started": "2025-05-05T09:52:05.415607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import shutil\n",
    "from os import makedirs\n",
    "from os.path import exists\n",
    "import re\n",
    "import string\n",
    "import struct\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import random as rand\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import operator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:52:05.421579Z",
     "iopub.status.busy": "2025-05-05T09:52:05.421302Z",
     "iopub.status.idle": "2025-05-05T09:52:09.907590Z",
     "shell.execute_reply": "2025-05-05T09:52:09.906970Z",
     "shell.execute_reply.started": "2025-05-05T09:52:05.421557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu train\n",
    "with open(\"/kaggle/input/code2que-data/Code2Que-data/javadata/src-train.txt\", 'r') as sf, \\\n",
    "     open(\"/kaggle/input/code2que-data/Code2Que-data/javadata/tgt-train.txt\", 'r') as tf:\n",
    "    data = [{\"source\": src.strip(), \"target\": tgt.strip()} for src, tgt in zip(sf, tf)]\n",
    "\n",
    "# Shuffle v√† split\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "split_idx = int(0.8 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "# Ghi file JSONL\n",
    "with open(\"train_split.json\", \"w\") as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "with open(\"val_split.json\", \"w\") as f:\n",
    "    for item in val_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# D·ªØ li·ªáu test\n",
    "with open(\"/kaggle/input/code2que-data/Code2Que-data/javadata/src-test-clear.txt\", 'r') as sf, \\\n",
    "     open(\"/kaggle/input/code2que-data/Code2Que-data/javadata/tgt-test-clear.txt\", 'r') as tf:\n",
    "    test_data = [{\"source\": src.strip(), \"target\": tgt.strip()} for src, tgt in zip(sf, tf)]\n",
    "\n",
    "with open(\"test.json\", \"w\") as f:\n",
    "    for item in test_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:52:09.908693Z",
     "iopub.status.busy": "2025-05-05T09:52:09.908417Z",
     "iopub.status.idle": "2025-05-05T09:52:12.913722Z",
     "shell.execute_reply": "2025-05-05T09:52:12.913121Z",
     "shell.execute_reply.started": "2025-05-05T09:52:09.908671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSONL (d·∫°ng m·ªói d√≤ng l√† m·ªôt dict)\n",
    "train_df = pd.read_json(\"train_split.json\", lines=True)\n",
    "val_df = pd.read_json(\"val_split.json\", lines=True)\n",
    "test_df = pd.read_json(\"test.json\", lines=True)\n",
    "\n",
    "# ‚úÖ Chuy·ªÉn sang Hugging Face Dataset, b·ªè index ƒë·ªÉ tr√°nh l·ªói\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(val_df, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_df, preserve_index=False)\n",
    "})\n",
    "\n",
    "# ‚úÖ Ch·ªçn t·∫≠p nh·ªè ƒë·ªÉ debug n·∫øu c·∫ßn\n",
    "small_dataset = DatasetDict({\n",
    "    \"train\": dataset[\"train\"].select(range(min(100, len(dataset[\"train\"])))),\n",
    "    \"validation\": dataset[\"validation\"].select(range(min(50, len(dataset[\"validation\"])))),\n",
    "    \"test\": dataset[\"test\"].select(range(min(50, len(dataset[\"test\"]))))\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:52:12.914588Z",
     "iopub.status.busy": "2025-05-05T09:52:12.914408Z",
     "iopub.status.idle": "2025-05-05T09:52:12.918221Z",
     "shell.execute_reply": "2025-05-05T09:52:12.917493Z",
     "shell.execute_reply.started": "2025-05-05T09:52:12.914574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#model_checkpoint = \"Salesforce/codet5-small\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:52:12.919100Z",
     "iopub.status.busy": "2025-05-05T09:52:12.918835Z",
     "iopub.status.idle": "2025-05-05T09:52:16.275331Z",
     "shell.execute_reply": "2025-05-05T09:52:16.274452Z",
     "shell.execute_reply.started": "2025-05-05T09:52:12.919042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274912ba8a0d43f9bf0a0cde680adc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6990d3ade8d744ad97c1b077f4a41b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623faa4f2a7f416392252cf3fc7c28ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24de1cd2a3df436baccf1a763b0dbc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e9c3adad614b5c9cc0c07981c0104f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c24d2d3a2eb45e2a5604f13fc41cca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e7c61b01d147dc943a5d49c8b360d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, PrefixTuningConfig, TaskType\n",
    "\n",
    "# üîπ T·∫£i m√¥ h√¨nh g·ªëc CodeT5-small\n",
    "model_checkpoint = \"Salesforce/codet5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:52:16.276813Z",
     "iopub.status.busy": "2025-05-05T09:52:16.276424Z",
     "iopub.status.idle": "2025-05-05T09:53:32.795013Z",
     "shell.execute_reply": "2025-05-05T09:53:32.794331Z",
     "shell.execute_reply.started": "2025-05-05T09:52:16.276785Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3aaa13955c4abaa59024a5c5cac6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33713baad080480db9c30424b042b1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0397fdba97c04f5eb0fd162fb8327eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2963 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331b8287b81f4dd6a7dbdae1e4b67123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a38098d8724e7eb951b92b100a3984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cdad50a0be4e3b962514b48586021b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Gh√©p prefix prompt v·ªõi code\n",
    "    inputs = [\"generate question: \" + code for code in examples['source']]\n",
    "#CodeT5 ho·∫°t ƒë·ªông t·ªët h∆°n n·∫øu c√≥ prompt ch·ªâ ƒë·ªãnh nhi·ªám v·ª•. => Hi·ªÉu y√™u c·∫ßu\n",
    "    # Tokenize input code (gi·ªõi h·∫°n max_length ƒë·ªÉ tr√°nh OOM)\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,           # ‚ö†Ô∏è gi·∫£m t·ª´ 256 ‚Üí 128\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "#M√£ h√≥a ƒëo·∫°n code th√†nh token ID.\n",
    "    # Tokenize target (t·ª©c c√¢u h·ªèi)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target\"],\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"]\n",
    ")\n",
    "\n",
    "tokenized_small_dataset = small_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:53:32.796997Z",
     "iopub.status.busy": "2025-05-05T09:53:32.796785Z",
     "iopub.status.idle": "2025-05-05T09:53:40.746273Z",
     "shell.execute_reply": "2025-05-05T09:53:40.745493Z",
     "shell.execute_reply.started": "2025-05-05T09:53:32.796981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuytran-soict-hust\u001b[0m (\u001b[33mhuytran-soict-hust-soict-hust\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"0947aabf2f78487e3d70ce59d03fd0e1e967672d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:53:40.747684Z",
     "iopub.status.busy": "2025-05-05T09:53:40.747100Z",
     "iopub.status.idle": "2025-05-05T09:53:40.752984Z",
     "shell.execute_reply": "2025-05-05T09:53:40.752436Z",
     "shell.execute_reply.started": "2025-05-05T09:53:40.747666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class PrintLossCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        epoch = int(state.epoch or 0)\n",
    "        train_loss = None\n",
    "        for log in reversed(state.log_history):\n",
    "            if \"loss\" in log:\n",
    "                train_loss = log[\"loss\"]\n",
    "                break\n",
    "\n",
    "        eval_loss = metrics.get(\"eval_loss\")\n",
    "        if eval_loss is not None:\n",
    "            print(f\"üìâ Epoch {epoch} - Train loss: {train_loss:.4f} | Eval loss: {eval_loss:.4f}\"\n",
    "                  if train_loss is not None else\n",
    "                  f\"üìâ Epoch {epoch} - Train loss: N/A | Eval loss: {eval_loss:.4f}\")\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:53:40.754033Z",
     "iopub.status.busy": "2025-05-05T09:53:40.753733Z",
     "iopub.status.idle": "2025-05-05T09:53:40.775623Z",
     "shell.execute_reply": "2025-05-05T09:53:40.774879Z",
     "shell.execute_reply.started": "2025-05-05T09:53:40.754004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# üîß Callback: Freeze to√†n b·ªô model (tr·ª´ lm_head) trong v√†i epoch ƒë·∫ßu\n",
    "class FreezeAndUnfreezeCallback(TrainerCallback):\n",
    "    def __init__(self, unfreeze_epoch=3):\n",
    "        self.unfreeze_epoch = unfreeze_epoch\n",
    "        self.frozen = False\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        if state.epoch < self.unfreeze_epoch and not self.frozen:\n",
    "            print(f\"üîí Freezing model until epoch {self.unfreeze_epoch} (Current: {state.epoch:.2f})\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if not name.startswith(\"lm_head\"):  # gi·ªØ lm_head ƒë·ªÉ train t·ª´ ƒë·∫ßu\n",
    "                    param.requires_grad = False\n",
    "            self.frozen = True\n",
    "        elif state.epoch >= self.unfreeze_epoch and self.frozen:\n",
    "            print(\"üîì Unfreezing all model weights\")\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "            self.frozen = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T10:02:58.205521Z",
     "iopub.status.busy": "2025-05-05T10:02:58.204896Z",
     "iopub.status.idle": "2025-05-05T10:02:58.244628Z",
     "shell.execute_reply": "2025-05-05T10:02:58.243943Z",
     "shell.execute_reply.started": "2025-05-05T10:02:58.205497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainerCallback\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainerCallback\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainerCallback\n",
    "\n",
    "# ‚úÖ C·∫•u h√¨nh hu·∫•n luy·ªán\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./codet5-finetuned-full\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=True, #mixed precision ƒë·ªÉ tƒÉng t·ªëc v√† ti·∫øt ki·ªám VRAM\n",
    "    load_best_model_at_end=False,\n",
    "    num_train_epochs=6,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"code2que-full-finetune\",\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=False,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "# ‚úÖ Callback log loss ƒë∆°n gi·∫£n\n",
    "class PrintLossCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            print(f\"üìâ Step {state.global_step}: loss = {logs['loss']:.4f}\")\n",
    "\n",
    "# ‚úÖ ƒê·∫£m b·∫£o m√¥ h√¨nh c√≥ th·ªÉ train to√†n b·ªô\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True  # Quan tr·ªçng!\n",
    "\n",
    "# ‚úÖ Hu·∫•n luy·ªán to√†n b·ªô m√¥ h√¨nh\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[PrintLossCallback()]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-05T14:32:29.920Z",
     "iopub.execute_input": "2025-05-05T10:03:07.986584Z",
     "iopub.status.busy": "2025-05-05T10:03:07.985932Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120812' max='150426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120812/150426 3:04:38 < 45:15, 10.90 it/s, Epoch 4.82/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.714200</td>\n",
       "      <td>0.687077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.669866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.655100</td>\n",
       "      <td>0.660727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.647900</td>\n",
       "      <td>0.656351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Step 100: loss = 0.7704\n",
      "üìâ Step 200: loss = 0.7635\n",
      "üìâ Step 300: loss = 0.7498\n",
      "üìâ Step 400: loss = 0.7474\n",
      "üìâ Step 500: loss = 0.7438\n",
      "üìâ Step 600: loss = 0.7513\n",
      "üìâ Step 700: loss = 0.7621\n",
      "üìâ Step 800: loss = 0.7538\n",
      "üìâ Step 900: loss = 0.7511\n",
      "üìâ Step 1000: loss = 0.7760\n",
      "üìâ Step 1100: loss = 0.7581\n",
      "üìâ Step 1200: loss = 0.7737\n",
      "üìâ Step 1300: loss = 0.7458\n",
      "üìâ Step 1400: loss = 0.7367\n",
      "üìâ Step 1500: loss = 0.7541\n",
      "üìâ Step 1600: loss = 0.7493\n",
      "üìâ Step 1700: loss = 0.7485\n",
      "üìâ Step 1800: loss = 0.7602\n",
      "üìâ Step 1900: loss = 0.7141\n",
      "üìâ Step 2000: loss = 0.7629\n",
      "üìâ Step 2100: loss = 0.7422\n",
      "üìâ Step 2200: loss = 0.7522\n",
      "üìâ Step 2300: loss = 0.7328\n",
      "üìâ Step 2400: loss = 0.7290\n",
      "üìâ Step 2500: loss = 0.7595\n",
      "üìâ Step 2600: loss = 0.7310\n",
      "üìâ Step 2700: loss = 0.7477\n",
      "üìâ Step 2800: loss = 0.7478\n",
      "üìâ Step 2900: loss = 0.7368\n",
      "üìâ Step 3000: loss = 0.7517\n",
      "üìâ Step 3100: loss = 0.7166\n",
      "üìâ Step 3200: loss = 0.7563\n",
      "üìâ Step 3300: loss = 0.7468\n",
      "üìâ Step 3400: loss = 0.7254\n",
      "üìâ Step 3500: loss = 0.7249\n",
      "üìâ Step 3600: loss = 0.7427\n",
      "üìâ Step 3700: loss = 0.7488\n",
      "üìâ Step 3800: loss = 0.7469\n",
      "üìâ Step 3900: loss = 0.7346\n",
      "üìâ Step 4000: loss = 0.7433\n",
      "üìâ Step 4100: loss = 0.7430\n",
      "üìâ Step 4200: loss = 0.7319\n",
      "üìâ Step 4300: loss = 0.7316\n",
      "üìâ Step 4400: loss = 0.7446\n",
      "üìâ Step 4500: loss = 0.7496\n",
      "üìâ Step 4600: loss = 0.7392\n",
      "üìâ Step 4700: loss = 0.7427\n",
      "üìâ Step 4800: loss = 0.7266\n",
      "üìâ Step 4900: loss = 0.7363\n",
      "üìâ Step 5000: loss = 0.7252\n",
      "üìâ Step 5100: loss = 0.7642\n",
      "üìâ Step 5200: loss = 0.7270\n",
      "üìâ Step 5300: loss = 0.7286\n",
      "üìâ Step 5400: loss = 0.7541\n",
      "üìâ Step 5500: loss = 0.7535\n",
      "üìâ Step 5600: loss = 0.7361\n",
      "üìâ Step 5700: loss = 0.7223\n",
      "üìâ Step 5800: loss = 0.7131\n",
      "üìâ Step 5900: loss = 0.7432\n",
      "üìâ Step 6000: loss = 0.7111\n",
      "üìâ Step 6100: loss = 0.7436\n",
      "üìâ Step 6200: loss = 0.7198\n",
      "üìâ Step 6300: loss = 0.7495\n",
      "üìâ Step 6400: loss = 0.7358\n",
      "üìâ Step 6500: loss = 0.7436\n",
      "üìâ Step 6600: loss = 0.7190\n",
      "üìâ Step 6700: loss = 0.7496\n",
      "üìâ Step 6800: loss = 0.7614\n",
      "üìâ Step 6900: loss = 0.7438\n",
      "üìâ Step 7000: loss = 0.7506\n",
      "üìâ Step 7100: loss = 0.7263\n",
      "üìâ Step 7200: loss = 0.7389\n",
      "üìâ Step 7300: loss = 0.7260\n",
      "üìâ Step 7400: loss = 0.7184\n",
      "üìâ Step 7500: loss = 0.7491\n",
      "üìâ Step 7600: loss = 0.7302\n",
      "üìâ Step 7700: loss = 0.7402\n",
      "üìâ Step 7800: loss = 0.7377\n",
      "üìâ Step 7900: loss = 0.7439\n",
      "üìâ Step 8000: loss = 0.7480\n",
      "üìâ Step 8100: loss = 0.7631\n",
      "üìâ Step 8200: loss = 0.7316\n",
      "üìâ Step 8300: loss = 0.7215\n",
      "üìâ Step 8400: loss = 0.7531\n",
      "üìâ Step 8500: loss = 0.7459\n",
      "üìâ Step 8600: loss = 0.7395\n",
      "üìâ Step 8700: loss = 0.7390\n",
      "üìâ Step 8800: loss = 0.7465\n",
      "üìâ Step 8900: loss = 0.7478\n",
      "üìâ Step 9000: loss = 0.7432\n",
      "üìâ Step 9100: loss = 0.7514\n",
      "üìâ Step 9200: loss = 0.7429\n",
      "üìâ Step 9300: loss = 0.7501\n",
      "üìâ Step 9400: loss = 0.7262\n",
      "üìâ Step 9500: loss = 0.7571\n",
      "üìâ Step 9600: loss = 0.7599\n",
      "üìâ Step 9700: loss = 0.7434\n",
      "üìâ Step 9800: loss = 0.7292\n",
      "üìâ Step 9900: loss = 0.7432\n",
      "üìâ Step 10000: loss = 0.7309\n",
      "üìâ Step 10100: loss = 0.7662\n",
      "üìâ Step 10200: loss = 0.7533\n",
      "üìâ Step 10300: loss = 0.7333\n",
      "üìâ Step 10400: loss = 0.7283\n",
      "üìâ Step 10500: loss = 0.7537\n",
      "üìâ Step 10600: loss = 0.7372\n",
      "üìâ Step 10700: loss = 0.7287\n",
      "üìâ Step 10800: loss = 0.7546\n",
      "üìâ Step 10900: loss = 0.7147\n",
      "üìâ Step 11000: loss = 0.7237\n",
      "üìâ Step 11100: loss = 0.7656\n",
      "üìâ Step 11200: loss = 0.7386\n",
      "üìâ Step 11300: loss = 0.7425\n",
      "üìâ Step 11400: loss = 0.7378\n",
      "üìâ Step 11500: loss = 0.7305\n",
      "üìâ Step 11600: loss = 0.7603\n",
      "üìâ Step 11700: loss = 0.7203\n",
      "üìâ Step 11800: loss = 0.7395\n",
      "üìâ Step 11900: loss = 0.7606\n",
      "üìâ Step 12000: loss = 0.7362\n",
      "üìâ Step 12100: loss = 0.7397\n",
      "üìâ Step 12200: loss = 0.7427\n",
      "üìâ Step 12300: loss = 0.7252\n",
      "üìâ Step 12400: loss = 0.7372\n",
      "üìâ Step 12500: loss = 0.7368\n",
      "üìâ Step 12600: loss = 0.7268\n",
      "üìâ Step 12700: loss = 0.7287\n",
      "üìâ Step 12800: loss = 0.7189\n",
      "üìâ Step 12900: loss = 0.7361\n",
      "üìâ Step 13000: loss = 0.7253\n",
      "üìâ Step 13100: loss = 0.7358\n",
      "üìâ Step 13200: loss = 0.7221\n",
      "üìâ Step 13300: loss = 0.7275\n",
      "üìâ Step 13400: loss = 0.7332\n",
      "üìâ Step 13500: loss = 0.7359\n",
      "üìâ Step 13600: loss = 0.7256\n",
      "üìâ Step 13700: loss = 0.7339\n",
      "üìâ Step 13800: loss = 0.7471\n",
      "üìâ Step 13900: loss = 0.7396\n",
      "üìâ Step 14000: loss = 0.7314\n",
      "üìâ Step 14100: loss = 0.7358\n",
      "üìâ Step 14200: loss = 0.7490\n",
      "üìâ Step 14300: loss = 0.7386\n",
      "üìâ Step 14400: loss = 0.7385\n",
      "üìâ Step 14500: loss = 0.7547\n",
      "üìâ Step 14600: loss = 0.7337\n",
      "üìâ Step 14700: loss = 0.7367\n",
      "üìâ Step 14800: loss = 0.6954\n",
      "üìâ Step 14900: loss = 0.7346\n",
      "üìâ Step 15000: loss = 0.7203\n",
      "üìâ Step 15100: loss = 0.7339\n",
      "üìâ Step 15200: loss = 0.7275\n",
      "üìâ Step 15300: loss = 0.7341\n",
      "üìâ Step 15400: loss = 0.7160\n",
      "üìâ Step 15500: loss = 0.7311\n",
      "üìâ Step 15600: loss = 0.7006\n",
      "üìâ Step 15700: loss = 0.7332\n",
      "üìâ Step 15800: loss = 0.7351\n",
      "üìâ Step 15900: loss = 0.7238\n",
      "üìâ Step 16000: loss = 0.7408\n",
      "üìâ Step 16100: loss = 0.7358\n",
      "üìâ Step 16200: loss = 0.7179\n",
      "üìâ Step 16300: loss = 0.7299\n",
      "üìâ Step 16400: loss = 0.6914\n",
      "üìâ Step 16500: loss = 0.7491\n",
      "üìâ Step 16600: loss = 0.6966\n",
      "üìâ Step 16700: loss = 0.7579\n",
      "üìâ Step 16800: loss = 0.7217\n",
      "üìâ Step 16900: loss = 0.7133\n",
      "üìâ Step 17000: loss = 0.7509\n",
      "üìâ Step 17100: loss = 0.7429\n",
      "üìâ Step 17200: loss = 0.7417\n",
      "üìâ Step 17300: loss = 0.7295\n",
      "üìâ Step 17400: loss = 0.7349\n",
      "üìâ Step 17500: loss = 0.7417\n",
      "üìâ Step 17600: loss = 0.7340\n",
      "üìâ Step 17700: loss = 0.7170\n",
      "üìâ Step 17800: loss = 0.7394\n",
      "üìâ Step 17900: loss = 0.7209\n",
      "üìâ Step 18000: loss = 0.7126\n",
      "üìâ Step 18100: loss = 0.7331\n",
      "üìâ Step 18200: loss = 0.7336\n",
      "üìâ Step 18300: loss = 0.7258\n",
      "üìâ Step 18400: loss = 0.7386\n",
      "üìâ Step 18500: loss = 0.7177\n",
      "üìâ Step 18600: loss = 0.7373\n",
      "üìâ Step 18700: loss = 0.7381\n",
      "üìâ Step 18800: loss = 0.7022\n",
      "üìâ Step 18900: loss = 0.7141\n",
      "üìâ Step 19000: loss = 0.7367\n",
      "üìâ Step 19100: loss = 0.7336\n",
      "üìâ Step 19200: loss = 0.7199\n",
      "üìâ Step 19300: loss = 0.7404\n",
      "üìâ Step 19400: loss = 0.7342\n",
      "üìâ Step 19500: loss = 0.7279\n",
      "üìâ Step 19600: loss = 0.7127\n",
      "üìâ Step 19700: loss = 0.7319\n",
      "üìâ Step 19800: loss = 0.7361\n",
      "üìâ Step 19900: loss = 0.7242\n",
      "üìâ Step 20000: loss = 0.7113\n",
      "üìâ Step 20100: loss = 0.7104\n",
      "üìâ Step 20200: loss = 0.7212\n",
      "üìâ Step 20300: loss = 0.7175\n",
      "üìâ Step 20400: loss = 0.7379\n",
      "üìâ Step 20500: loss = 0.7223\n",
      "üìâ Step 20600: loss = 0.6993\n",
      "üìâ Step 20700: loss = 0.7327\n",
      "üìâ Step 20800: loss = 0.7251\n",
      "üìâ Step 20900: loss = 0.7170\n",
      "üìâ Step 21000: loss = 0.7261\n",
      "üìâ Step 21100: loss = 0.7178\n",
      "üìâ Step 21200: loss = 0.7064\n",
      "üìâ Step 21300: loss = 0.7226\n",
      "üìâ Step 21400: loss = 0.7136\n",
      "üìâ Step 21500: loss = 0.7325\n",
      "üìâ Step 21600: loss = 0.7148\n",
      "üìâ Step 21700: loss = 0.7337\n",
      "üìâ Step 21800: loss = 0.7179\n",
      "üìâ Step 21900: loss = 0.7140\n",
      "üìâ Step 22000: loss = 0.7292\n",
      "üìâ Step 22100: loss = 0.7209\n",
      "üìâ Step 22200: loss = 0.7591\n",
      "üìâ Step 22300: loss = 0.7266\n",
      "üìâ Step 22400: loss = 0.7356\n",
      "üìâ Step 22500: loss = 0.7394\n",
      "üìâ Step 22600: loss = 0.7196\n",
      "üìâ Step 22700: loss = 0.7320\n",
      "üìâ Step 22800: loss = 0.7352\n",
      "üìâ Step 22900: loss = 0.7238\n",
      "üìâ Step 23000: loss = 0.7204\n",
      "üìâ Step 23100: loss = 0.7248\n",
      "üìâ Step 23200: loss = 0.7126\n",
      "üìâ Step 23300: loss = 0.7035\n",
      "üìâ Step 23400: loss = 0.7170\n",
      "üìâ Step 23500: loss = 0.7263\n",
      "üìâ Step 23600: loss = 0.7220\n",
      "üìâ Step 23700: loss = 0.7294\n",
      "üìâ Step 23800: loss = 0.7059\n",
      "üìâ Step 23900: loss = 0.7171\n",
      "üìâ Step 24000: loss = 0.7425\n",
      "üìâ Step 24100: loss = 0.7105\n",
      "üìâ Step 24200: loss = 0.7175\n",
      "üìâ Step 24300: loss = 0.6931\n",
      "üìâ Step 24400: loss = 0.7153\n",
      "üìâ Step 24500: loss = 0.7007\n",
      "üìâ Step 24600: loss = 0.7497\n",
      "üìâ Step 24700: loss = 0.7167\n",
      "üìâ Step 24800: loss = 0.7214\n",
      "üìâ Step 24900: loss = 0.7109\n",
      "üìâ Step 25000: loss = 0.7142\n",
      "üìâ Step 25100: loss = 0.6913\n",
      "üìâ Step 25200: loss = 0.7044\n",
      "üìâ Step 25300: loss = 0.7017\n",
      "üìâ Step 25400: loss = 0.6920\n",
      "üìâ Step 25500: loss = 0.6725\n",
      "üìâ Step 25600: loss = 0.6842\n",
      "üìâ Step 25700: loss = 0.6819\n",
      "üìâ Step 25800: loss = 0.6913\n",
      "üìâ Step 25900: loss = 0.7248\n",
      "üìâ Step 26000: loss = 0.6914\n",
      "üìâ Step 26100: loss = 0.6923\n",
      "üìâ Step 26200: loss = 0.6959\n",
      "üìâ Step 26300: loss = 0.6946\n",
      "üìâ Step 26400: loss = 0.6948\n",
      "üìâ Step 26500: loss = 0.6983\n",
      "üìâ Step 26600: loss = 0.7041\n",
      "üìâ Step 26700: loss = 0.7063\n",
      "üìâ Step 26800: loss = 0.6985\n",
      "üìâ Step 26900: loss = 0.6864\n",
      "üìâ Step 27000: loss = 0.7066\n",
      "üìâ Step 27100: loss = 0.6835\n",
      "üìâ Step 27200: loss = 0.6927\n",
      "üìâ Step 27300: loss = 0.6947\n",
      "üìâ Step 27400: loss = 0.6730\n",
      "üìâ Step 27500: loss = 0.6886\n",
      "üìâ Step 27600: loss = 0.7110\n",
      "üìâ Step 27700: loss = 0.7164\n",
      "üìâ Step 27800: loss = 0.6972\n",
      "üìâ Step 27900: loss = 0.6851\n",
      "üìâ Step 28000: loss = 0.7162\n",
      "üìâ Step 28100: loss = 0.6933\n",
      "üìâ Step 28200: loss = 0.6917\n",
      "üìâ Step 28300: loss = 0.6816\n",
      "üìâ Step 28400: loss = 0.6963\n",
      "üìâ Step 28500: loss = 0.6872\n",
      "üìâ Step 28600: loss = 0.7020\n",
      "üìâ Step 28700: loss = 0.6869\n",
      "üìâ Step 28800: loss = 0.6669\n",
      "üìâ Step 28900: loss = 0.6930\n",
      "üìâ Step 29000: loss = 0.6927\n",
      "üìâ Step 29100: loss = 0.6729\n",
      "üìâ Step 29200: loss = 0.6915\n",
      "üìâ Step 29300: loss = 0.6629\n",
      "üìâ Step 29400: loss = 0.6788\n",
      "üìâ Step 29500: loss = 0.6985\n",
      "üìâ Step 29600: loss = 0.6827\n",
      "üìâ Step 29700: loss = 0.6808\n",
      "üìâ Step 29800: loss = 0.6903\n",
      "üìâ Step 29900: loss = 0.7064\n",
      "üìâ Step 30000: loss = 0.7021\n",
      "üìâ Step 30100: loss = 0.6916\n",
      "üìâ Step 30200: loss = 0.6796\n",
      "üìâ Step 30300: loss = 0.6963\n",
      "üìâ Step 30400: loss = 0.6854\n",
      "üìâ Step 30500: loss = 0.6840\n",
      "üìâ Step 30600: loss = 0.6957\n",
      "üìâ Step 30700: loss = 0.6931\n",
      "üìâ Step 30800: loss = 0.6640\n",
      "üìâ Step 30900: loss = 0.6926\n",
      "üìâ Step 31000: loss = 0.6811\n",
      "üìâ Step 31100: loss = 0.6803\n",
      "üìâ Step 31200: loss = 0.7055\n",
      "üìâ Step 31300: loss = 0.6936\n",
      "üìâ Step 31400: loss = 0.6684\n",
      "üìâ Step 31500: loss = 0.6917\n",
      "üìâ Step 31600: loss = 0.6941\n",
      "üìâ Step 31700: loss = 0.6980\n",
      "üìâ Step 31800: loss = 0.6906\n",
      "üìâ Step 31900: loss = 0.6938\n",
      "üìâ Step 32000: loss = 0.7075\n",
      "üìâ Step 32100: loss = 0.6720\n",
      "üìâ Step 32200: loss = 0.6912\n",
      "üìâ Step 32300: loss = 0.6892\n",
      "üìâ Step 32400: loss = 0.7224\n",
      "üìâ Step 32500: loss = 0.7071\n",
      "üìâ Step 32600: loss = 0.6833\n",
      "üìâ Step 32700: loss = 0.7026\n",
      "üìâ Step 32800: loss = 0.6893\n",
      "üìâ Step 32900: loss = 0.7161\n",
      "üìâ Step 33000: loss = 0.6804\n",
      "üìâ Step 33100: loss = 0.7156\n",
      "üìâ Step 33200: loss = 0.6806\n",
      "üìâ Step 33300: loss = 0.6990\n",
      "üìâ Step 33400: loss = 0.6949\n",
      "üìâ Step 33500: loss = 0.7016\n",
      "üìâ Step 33600: loss = 0.6835\n",
      "üìâ Step 33700: loss = 0.6745\n",
      "üìâ Step 33800: loss = 0.6730\n",
      "üìâ Step 33900: loss = 0.6811\n",
      "üìâ Step 34000: loss = 0.6843\n",
      "üìâ Step 34100: loss = 0.6927\n",
      "üìâ Step 34200: loss = 0.7061\n",
      "üìâ Step 34300: loss = 0.6928\n",
      "üìâ Step 34400: loss = 0.6697\n",
      "üìâ Step 34500: loss = 0.6994\n",
      "üìâ Step 34600: loss = 0.6996\n",
      "üìâ Step 34700: loss = 0.7000\n",
      "üìâ Step 34800: loss = 0.6713\n",
      "üìâ Step 34900: loss = 0.6627\n",
      "üìâ Step 35000: loss = 0.6983\n",
      "üìâ Step 35100: loss = 0.6868\n",
      "üìâ Step 35200: loss = 0.6771\n",
      "üìâ Step 35300: loss = 0.7152\n",
      "üìâ Step 35400: loss = 0.6989\n",
      "üìâ Step 35500: loss = 0.7014\n",
      "üìâ Step 35600: loss = 0.6851\n",
      "üìâ Step 35700: loss = 0.6931\n",
      "üìâ Step 35800: loss = 0.6899\n",
      "üìâ Step 35900: loss = 0.6638\n",
      "üìâ Step 36000: loss = 0.7035\n",
      "üìâ Step 36100: loss = 0.6867\n",
      "üìâ Step 36200: loss = 0.6724\n",
      "üìâ Step 36300: loss = 0.6944\n",
      "üìâ Step 36400: loss = 0.6816\n",
      "üìâ Step 36500: loss = 0.7017\n",
      "üìâ Step 36600: loss = 0.6754\n",
      "üìâ Step 36700: loss = 0.6903\n",
      "üìâ Step 36800: loss = 0.6952\n",
      "üìâ Step 36900: loss = 0.6732\n",
      "üìâ Step 37000: loss = 0.6787\n",
      "üìâ Step 37100: loss = 0.7046\n",
      "üìâ Step 37200: loss = 0.6734\n",
      "üìâ Step 37300: loss = 0.6819\n",
      "üìâ Step 37400: loss = 0.6766\n",
      "üìâ Step 37500: loss = 0.6743\n",
      "üìâ Step 37600: loss = 0.6971\n",
      "üìâ Step 37700: loss = 0.6734\n",
      "üìâ Step 37800: loss = 0.6753\n",
      "üìâ Step 37900: loss = 0.6845\n",
      "üìâ Step 38000: loss = 0.6782\n",
      "üìâ Step 38100: loss = 0.6853\n",
      "üìâ Step 38200: loss = 0.6863\n",
      "üìâ Step 38300: loss = 0.6789\n",
      "üìâ Step 38400: loss = 0.6873\n",
      "üìâ Step 38500: loss = 0.6953\n",
      "üìâ Step 38600: loss = 0.6847\n",
      "üìâ Step 38700: loss = 0.6864\n",
      "üìâ Step 38800: loss = 0.6713\n",
      "üìâ Step 38900: loss = 0.6963\n",
      "üìâ Step 39000: loss = 0.6898\n",
      "üìâ Step 39100: loss = 0.7043\n",
      "üìâ Step 39200: loss = 0.6912\n",
      "üìâ Step 39300: loss = 0.6767\n",
      "üìâ Step 39400: loss = 0.6781\n",
      "üìâ Step 39500: loss = 0.6931\n",
      "üìâ Step 39600: loss = 0.6916\n",
      "üìâ Step 39700: loss = 0.6831\n",
      "üìâ Step 39800: loss = 0.6804\n",
      "üìâ Step 39900: loss = 0.7010\n",
      "üìâ Step 40000: loss = 0.6867\n",
      "üìâ Step 40100: loss = 0.6678\n",
      "üìâ Step 40200: loss = 0.6861\n",
      "üìâ Step 40300: loss = 0.6792\n",
      "üìâ Step 40400: loss = 0.6869\n",
      "üìâ Step 40500: loss = 0.6947\n",
      "üìâ Step 40600: loss = 0.6701\n",
      "üìâ Step 40700: loss = 0.6758\n",
      "üìâ Step 40800: loss = 0.6906\n",
      "üìâ Step 40900: loss = 0.7107\n",
      "üìâ Step 41000: loss = 0.7096\n",
      "üìâ Step 41100: loss = 0.6936\n",
      "üìâ Step 41200: loss = 0.6977\n",
      "üìâ Step 41300: loss = 0.7015\n",
      "üìâ Step 41400: loss = 0.6955\n",
      "üìâ Step 41500: loss = 0.6981\n",
      "üìâ Step 41600: loss = 0.7019\n",
      "üìâ Step 41700: loss = 0.6942\n",
      "üìâ Step 41800: loss = 0.6981\n",
      "üìâ Step 41900: loss = 0.6841\n",
      "üìâ Step 42000: loss = 0.6740\n",
      "üìâ Step 42100: loss = 0.6928\n",
      "üìâ Step 42200: loss = 0.6850\n",
      "üìâ Step 42300: loss = 0.6897\n",
      "üìâ Step 42400: loss = 0.6786\n",
      "üìâ Step 42500: loss = 0.6643\n",
      "üìâ Step 42600: loss = 0.6814\n",
      "üìâ Step 42700: loss = 0.6863\n",
      "üìâ Step 42800: loss = 0.7027\n",
      "üìâ Step 42900: loss = 0.6850\n",
      "üìâ Step 43000: loss = 0.7040\n",
      "üìâ Step 43100: loss = 0.6814\n",
      "üìâ Step 43200: loss = 0.7050\n",
      "üìâ Step 43300: loss = 0.6924\n",
      "üìâ Step 43400: loss = 0.6735\n",
      "üìâ Step 43500: loss = 0.6827\n",
      "üìâ Step 43600: loss = 0.6920\n",
      "üìâ Step 43700: loss = 0.6865\n",
      "üìâ Step 43800: loss = 0.6808\n",
      "üìâ Step 43900: loss = 0.6871\n",
      "üìâ Step 44000: loss = 0.6668\n",
      "üìâ Step 44100: loss = 0.6834\n",
      "üìâ Step 44200: loss = 0.6852\n",
      "üìâ Step 44300: loss = 0.6789\n",
      "üìâ Step 44400: loss = 0.6913\n",
      "üìâ Step 44500: loss = 0.6787\n",
      "üìâ Step 44600: loss = 0.6767\n",
      "üìâ Step 44700: loss = 0.6634\n",
      "üìâ Step 44800: loss = 0.6726\n",
      "üìâ Step 44900: loss = 0.6885\n",
      "üìâ Step 45000: loss = 0.6871\n",
      "üìâ Step 45100: loss = 0.6922\n",
      "üìâ Step 45200: loss = 0.6659\n",
      "üìâ Step 45300: loss = 0.6923\n",
      "üìâ Step 45400: loss = 0.6812\n",
      "üìâ Step 45500: loss = 0.6769\n",
      "üìâ Step 45600: loss = 0.6798\n",
      "üìâ Step 45700: loss = 0.6819\n",
      "üìâ Step 45800: loss = 0.6884\n",
      "üìâ Step 45900: loss = 0.6836\n",
      "üìâ Step 46000: loss = 0.6778\n",
      "üìâ Step 46100: loss = 0.7101\n",
      "üìâ Step 46200: loss = 0.6736\n",
      "üìâ Step 46300: loss = 0.6767\n",
      "üìâ Step 46400: loss = 0.7002\n",
      "üìâ Step 46500: loss = 0.6733\n",
      "üìâ Step 46600: loss = 0.6909\n",
      "üìâ Step 46700: loss = 0.7030\n",
      "üìâ Step 46800: loss = 0.6883\n",
      "üìâ Step 46900: loss = 0.6717\n",
      "üìâ Step 47000: loss = 0.6825\n",
      "üìâ Step 47100: loss = 0.6757\n",
      "üìâ Step 47200: loss = 0.6907\n",
      "üìâ Step 47300: loss = 0.7013\n",
      "üìâ Step 47400: loss = 0.6768\n",
      "üìâ Step 47500: loss = 0.6896\n",
      "üìâ Step 47600: loss = 0.7028\n",
      "üìâ Step 47700: loss = 0.6878\n",
      "üìâ Step 47800: loss = 0.6580\n",
      "üìâ Step 47900: loss = 0.6776\n",
      "üìâ Step 48000: loss = 0.6878\n",
      "üìâ Step 48100: loss = 0.7001\n",
      "üìâ Step 48200: loss = 0.6860\n",
      "üìâ Step 48300: loss = 0.6726\n",
      "üìâ Step 48400: loss = 0.6972\n",
      "üìâ Step 48500: loss = 0.6853\n",
      "üìâ Step 48600: loss = 0.6878\n",
      "üìâ Step 48700: loss = 0.6868\n",
      "üìâ Step 48800: loss = 0.6622\n",
      "üìâ Step 48900: loss = 0.6693\n",
      "üìâ Step 49000: loss = 0.6750\n",
      "üìâ Step 49100: loss = 0.7031\n",
      "üìâ Step 49200: loss = 0.6811\n",
      "üìâ Step 49300: loss = 0.6734\n",
      "üìâ Step 49400: loss = 0.6821\n",
      "üìâ Step 49500: loss = 0.6873\n",
      "üìâ Step 49600: loss = 0.6619\n",
      "üìâ Step 49700: loss = 0.6693\n",
      "üìâ Step 49800: loss = 0.6822\n",
      "üìâ Step 49900: loss = 0.6777\n",
      "üìâ Step 50000: loss = 0.6669\n",
      "üìâ Step 50100: loss = 0.6896\n",
      "üìâ Step 50200: loss = 0.6697\n",
      "üìâ Step 50300: loss = 0.6617\n",
      "üìâ Step 50400: loss = 0.6573\n",
      "üìâ Step 50500: loss = 0.6718\n",
      "üìâ Step 50600: loss = 0.6438\n",
      "üìâ Step 50700: loss = 0.6565\n",
      "üìâ Step 50800: loss = 0.6598\n",
      "üìâ Step 50900: loss = 0.6560\n",
      "üìâ Step 51000: loss = 0.6748\n",
      "üìâ Step 51100: loss = 0.6591\n",
      "üìâ Step 51200: loss = 0.6647\n",
      "üìâ Step 51300: loss = 0.6767\n",
      "üìâ Step 51400: loss = 0.6536\n",
      "üìâ Step 51500: loss = 0.6685\n",
      "üìâ Step 51600: loss = 0.6774\n",
      "üìâ Step 51700: loss = 0.6678\n",
      "üìâ Step 51800: loss = 0.6547\n",
      "üìâ Step 51900: loss = 0.6319\n",
      "üìâ Step 52000: loss = 0.6637\n",
      "üìâ Step 52100: loss = 0.6894\n",
      "üìâ Step 52200: loss = 0.6662\n",
      "üìâ Step 52300: loss = 0.6585\n",
      "üìâ Step 52400: loss = 0.6518\n",
      "üìâ Step 52500: loss = 0.6612\n",
      "üìâ Step 52600: loss = 0.6538\n",
      "üìâ Step 52700: loss = 0.6525\n",
      "üìâ Step 52800: loss = 0.6706\n",
      "üìâ Step 52900: loss = 0.6752\n",
      "üìâ Step 53000: loss = 0.6661\n",
      "üìâ Step 53100: loss = 0.6529\n",
      "üìâ Step 53200: loss = 0.6663\n",
      "üìâ Step 53300: loss = 0.6566\n",
      "üìâ Step 53400: loss = 0.6924\n",
      "üìâ Step 53500: loss = 0.6597\n",
      "üìâ Step 53600: loss = 0.6553\n",
      "üìâ Step 53700: loss = 0.6644\n",
      "üìâ Step 53800: loss = 0.6599\n",
      "üìâ Step 53900: loss = 0.6554\n",
      "üìâ Step 54000: loss = 0.6800\n",
      "üìâ Step 54100: loss = 0.6537\n",
      "üìâ Step 54200: loss = 0.6554\n",
      "üìâ Step 54300: loss = 0.6628\n",
      "üìâ Step 54400: loss = 0.6580\n",
      "üìâ Step 54500: loss = 0.6640\n",
      "üìâ Step 54600: loss = 0.6888\n",
      "üìâ Step 54700: loss = 0.6606\n",
      "üìâ Step 54800: loss = 0.6622\n",
      "üìâ Step 54900: loss = 0.6617\n",
      "üìâ Step 55000: loss = 0.6746\n",
      "üìâ Step 55100: loss = 0.6577\n",
      "üìâ Step 55200: loss = 0.6683\n",
      "üìâ Step 55300: loss = 0.6527\n",
      "üìâ Step 55400: loss = 0.6828\n",
      "üìâ Step 55500: loss = 0.6505\n",
      "üìâ Step 55600: loss = 0.6604\n",
      "üìâ Step 55700: loss = 0.6651\n",
      "üìâ Step 55800: loss = 0.6403\n",
      "üìâ Step 55900: loss = 0.6559\n",
      "üìâ Step 56000: loss = 0.6496\n",
      "üìâ Step 56100: loss = 0.6648\n",
      "üìâ Step 56200: loss = 0.6535\n",
      "üìâ Step 56300: loss = 0.6593\n",
      "üìâ Step 56400: loss = 0.6725\n",
      "üìâ Step 56500: loss = 0.6615\n",
      "üìâ Step 56600: loss = 0.6712\n",
      "üìâ Step 56700: loss = 0.6630\n",
      "üìâ Step 56800: loss = 0.6495\n",
      "üìâ Step 56900: loss = 0.6757\n",
      "üìâ Step 57000: loss = 0.6562\n",
      "üìâ Step 57100: loss = 0.6533\n",
      "üìâ Step 57200: loss = 0.6744\n",
      "üìâ Step 57300: loss = 0.6463\n",
      "üìâ Step 57400: loss = 0.6822\n",
      "üìâ Step 57500: loss = 0.6554\n",
      "üìâ Step 57600: loss = 0.6685\n",
      "üìâ Step 57700: loss = 0.6585\n",
      "üìâ Step 57800: loss = 0.6624\n",
      "üìâ Step 57900: loss = 0.6619\n",
      "üìâ Step 58000: loss = 0.6639\n",
      "üìâ Step 58100: loss = 0.6675\n",
      "üìâ Step 58200: loss = 0.6703\n",
      "üìâ Step 58300: loss = 0.6580\n",
      "üìâ Step 58400: loss = 0.6584\n",
      "üìâ Step 58500: loss = 0.6745\n",
      "üìâ Step 58600: loss = 0.6379\n",
      "üìâ Step 58700: loss = 0.6550\n",
      "üìâ Step 58800: loss = 0.6619\n",
      "üìâ Step 58900: loss = 0.6567\n",
      "üìâ Step 59000: loss = 0.6544\n",
      "üìâ Step 59100: loss = 0.6582\n",
      "üìâ Step 59200: loss = 0.6615\n",
      "üìâ Step 59300: loss = 0.6656\n",
      "üìâ Step 59400: loss = 0.6753\n",
      "üìâ Step 59500: loss = 0.6741\n",
      "üìâ Step 59600: loss = 0.6543\n",
      "üìâ Step 59700: loss = 0.6633\n",
      "üìâ Step 59800: loss = 0.6503\n",
      "üìâ Step 59900: loss = 0.6693\n",
      "üìâ Step 60000: loss = 0.6486\n",
      "üìâ Step 60100: loss = 0.6543\n",
      "üìâ Step 60200: loss = 0.6568\n",
      "üìâ Step 60300: loss = 0.6647\n",
      "üìâ Step 60400: loss = 0.6691\n",
      "üìâ Step 60500: loss = 0.6561\n",
      "üìâ Step 60600: loss = 0.6766\n",
      "üìâ Step 60700: loss = 0.6706\n",
      "üìâ Step 60800: loss = 0.6672\n",
      "üìâ Step 60900: loss = 0.6701\n",
      "üìâ Step 61000: loss = 0.6386\n",
      "üìâ Step 61100: loss = 0.6744\n",
      "üìâ Step 61200: loss = 0.6638\n",
      "üìâ Step 61300: loss = 0.6686\n",
      "üìâ Step 61400: loss = 0.6676\n",
      "üìâ Step 61500: loss = 0.6731\n",
      "üìâ Step 61600: loss = 0.6467\n",
      "üìâ Step 61700: loss = 0.6673\n",
      "üìâ Step 61800: loss = 0.6609\n",
      "üìâ Step 61900: loss = 0.6481\n",
      "üìâ Step 62000: loss = 0.6539\n",
      "üìâ Step 62100: loss = 0.6667\n",
      "üìâ Step 62200: loss = 0.6618\n",
      "üìâ Step 62300: loss = 0.6600\n",
      "üìâ Step 62400: loss = 0.6321\n",
      "üìâ Step 62500: loss = 0.6547\n",
      "üìâ Step 62600: loss = 0.6380\n",
      "üìâ Step 62700: loss = 0.6444\n",
      "üìâ Step 62800: loss = 0.6791\n",
      "üìâ Step 62900: loss = 0.6621\n",
      "üìâ Step 63000: loss = 0.6609\n",
      "üìâ Step 63100: loss = 0.6595\n",
      "üìâ Step 63200: loss = 0.6759\n",
      "üìâ Step 63300: loss = 0.6631\n",
      "üìâ Step 63400: loss = 0.6677\n",
      "üìâ Step 63500: loss = 0.6500\n",
      "üìâ Step 63600: loss = 0.6709\n",
      "üìâ Step 63700: loss = 0.6645\n",
      "üìâ Step 63800: loss = 0.6657\n",
      "üìâ Step 63900: loss = 0.6706\n",
      "üìâ Step 64000: loss = 0.6684\n",
      "üìâ Step 64100: loss = 0.6590\n",
      "üìâ Step 64200: loss = 0.6427\n",
      "üìâ Step 64300: loss = 0.6492\n",
      "üìâ Step 64400: loss = 0.6686\n",
      "üìâ Step 64500: loss = 0.6648\n",
      "üìâ Step 64600: loss = 0.6713\n",
      "üìâ Step 64700: loss = 0.6650\n",
      "üìâ Step 64800: loss = 0.6638\n",
      "üìâ Step 64900: loss = 0.6560\n",
      "üìâ Step 65000: loss = 0.6507\n",
      "üìâ Step 65100: loss = 0.6659\n",
      "üìâ Step 65200: loss = 0.6499\n",
      "üìâ Step 65300: loss = 0.6468\n",
      "üìâ Step 65400: loss = 0.6506\n",
      "üìâ Step 65500: loss = 0.6442\n",
      "üìâ Step 65600: loss = 0.6596\n",
      "üìâ Step 65700: loss = 0.6764\n",
      "üìâ Step 65800: loss = 0.6727\n",
      "üìâ Step 65900: loss = 0.6585\n",
      "üìâ Step 66000: loss = 0.6478\n",
      "üìâ Step 66100: loss = 0.6626\n",
      "üìâ Step 66200: loss = 0.6570\n",
      "üìâ Step 66300: loss = 0.6784\n",
      "üìâ Step 66400: loss = 0.6551\n",
      "üìâ Step 66500: loss = 0.6568\n",
      "üìâ Step 66600: loss = 0.6625\n",
      "üìâ Step 66700: loss = 0.6837\n",
      "üìâ Step 66800: loss = 0.6427\n",
      "üìâ Step 66900: loss = 0.6872\n",
      "üìâ Step 67000: loss = 0.6678\n",
      "üìâ Step 67100: loss = 0.6716\n",
      "üìâ Step 67200: loss = 0.6702\n",
      "üìâ Step 67300: loss = 0.6548\n",
      "üìâ Step 67400: loss = 0.6521\n",
      "üìâ Step 67500: loss = 0.6587\n",
      "üìâ Step 67600: loss = 0.6701\n",
      "üìâ Step 67700: loss = 0.6797\n",
      "üìâ Step 67800: loss = 0.6702\n",
      "üìâ Step 67900: loss = 0.6519\n",
      "üìâ Step 68000: loss = 0.6546\n",
      "üìâ Step 68100: loss = 0.6681\n",
      "üìâ Step 68200: loss = 0.6620\n",
      "üìâ Step 68300: loss = 0.6542\n",
      "üìâ Step 68400: loss = 0.6285\n",
      "üìâ Step 68500: loss = 0.6591\n",
      "üìâ Step 68600: loss = 0.6521\n",
      "üìâ Step 68700: loss = 0.6720\n",
      "üìâ Step 68800: loss = 0.6594\n",
      "üìâ Step 68900: loss = 0.6777\n",
      "üìâ Step 69000: loss = 0.6706\n",
      "üìâ Step 69100: loss = 0.6630\n",
      "üìâ Step 69200: loss = 0.6639\n",
      "üìâ Step 69300: loss = 0.6671\n",
      "üìâ Step 69400: loss = 0.6552\n",
      "üìâ Step 69500: loss = 0.6522\n",
      "üìâ Step 69600: loss = 0.6676\n",
      "üìâ Step 69700: loss = 0.6623\n",
      "üìâ Step 69800: loss = 0.6602\n",
      "üìâ Step 69900: loss = 0.6492\n",
      "üìâ Step 70000: loss = 0.6654\n",
      "üìâ Step 70100: loss = 0.6649\n",
      "üìâ Step 70200: loss = 0.6578\n",
      "üìâ Step 70300: loss = 0.6644\n",
      "üìâ Step 70400: loss = 0.6967\n",
      "üìâ Step 70500: loss = 0.6683\n",
      "üìâ Step 70600: loss = 0.6624\n",
      "üìâ Step 70700: loss = 0.6531\n",
      "üìâ Step 70800: loss = 0.6447\n",
      "üìâ Step 70900: loss = 0.6579\n",
      "üìâ Step 71000: loss = 0.6565\n",
      "üìâ Step 71100: loss = 0.6660\n",
      "üìâ Step 71200: loss = 0.6551\n",
      "üìâ Step 71300: loss = 0.6600\n",
      "üìâ Step 71400: loss = 0.6513\n",
      "üìâ Step 71500: loss = 0.6629\n",
      "üìâ Step 71600: loss = 0.6655\n",
      "üìâ Step 71700: loss = 0.6580\n",
      "üìâ Step 71800: loss = 0.6497\n",
      "üìâ Step 71900: loss = 0.6670\n",
      "üìâ Step 72000: loss = 0.6503\n",
      "üìâ Step 72100: loss = 0.6455\n",
      "üìâ Step 72200: loss = 0.6783\n",
      "üìâ Step 72300: loss = 0.6719\n",
      "üìâ Step 72400: loss = 0.6560\n",
      "üìâ Step 72500: loss = 0.6748\n",
      "üìâ Step 72600: loss = 0.6408\n",
      "üìâ Step 72700: loss = 0.6390\n",
      "üìâ Step 72800: loss = 0.6703\n",
      "üìâ Step 72900: loss = 0.6699\n",
      "üìâ Step 73000: loss = 0.6649\n",
      "üìâ Step 73100: loss = 0.6538\n",
      "üìâ Step 73200: loss = 0.6693\n",
      "üìâ Step 73300: loss = 0.6473\n",
      "üìâ Step 73400: loss = 0.6722\n",
      "üìâ Step 73500: loss = 0.6567\n",
      "üìâ Step 73600: loss = 0.6644\n",
      "üìâ Step 73700: loss = 0.6609\n",
      "üìâ Step 73800: loss = 0.6374\n",
      "üìâ Step 73900: loss = 0.6351\n",
      "üìâ Step 74000: loss = 0.6686\n",
      "üìâ Step 74100: loss = 0.6748\n",
      "üìâ Step 74200: loss = 0.6468\n",
      "üìâ Step 74300: loss = 0.6527\n",
      "üìâ Step 74400: loss = 0.6705\n",
      "üìâ Step 74500: loss = 0.6448\n",
      "üìâ Step 74600: loss = 0.6699\n",
      "üìâ Step 74700: loss = 0.6837\n",
      "üìâ Step 74800: loss = 0.6841\n",
      "üìâ Step 74900: loss = 0.6502\n",
      "üìâ Step 75000: loss = 0.6717\n",
      "üìâ Step 75100: loss = 0.6486\n",
      "üìâ Step 75200: loss = 0.6551\n",
      "üìâ Step 75300: loss = 0.6436\n",
      "üìâ Step 75400: loss = 0.6434\n",
      "üìâ Step 75500: loss = 0.6230\n",
      "üìâ Step 75600: loss = 0.6389\n",
      "üìâ Step 75700: loss = 0.6509\n",
      "üìâ Step 75800: loss = 0.6468\n",
      "üìâ Step 75900: loss = 0.6145\n",
      "üìâ Step 76000: loss = 0.6427\n",
      "üìâ Step 76100: loss = 0.6366\n",
      "üìâ Step 76200: loss = 0.6257\n",
      "üìâ Step 76300: loss = 0.6343\n",
      "üìâ Step 76400: loss = 0.6339\n",
      "üìâ Step 76500: loss = 0.6390\n",
      "üìâ Step 76600: loss = 0.6598\n",
      "üìâ Step 76700: loss = 0.6494\n",
      "üìâ Step 76800: loss = 0.6428\n",
      "üìâ Step 76900: loss = 0.6258\n",
      "üìâ Step 77000: loss = 0.6362\n",
      "üìâ Step 77100: loss = 0.6381\n",
      "üìâ Step 77200: loss = 0.6477\n",
      "üìâ Step 77300: loss = 0.6360\n",
      "üìâ Step 77400: loss = 0.6465\n",
      "üìâ Step 77500: loss = 0.6382\n",
      "üìâ Step 77600: loss = 0.6419\n",
      "üìâ Step 77700: loss = 0.6404\n",
      "üìâ Step 77800: loss = 0.6534\n",
      "üìâ Step 77900: loss = 0.6484\n",
      "üìâ Step 78000: loss = 0.6289\n",
      "üìâ Step 78100: loss = 0.6377\n",
      "üìâ Step 78200: loss = 0.6483\n",
      "üìâ Step 78300: loss = 0.6167\n",
      "üìâ Step 78400: loss = 0.6260\n",
      "üìâ Step 78500: loss = 0.6279\n",
      "üìâ Step 78600: loss = 0.6559\n",
      "üìâ Step 78700: loss = 0.6297\n",
      "üìâ Step 78800: loss = 0.6506\n",
      "üìâ Step 78900: loss = 0.6422\n",
      "üìâ Step 79000: loss = 0.6355\n",
      "üìâ Step 79100: loss = 0.6383\n",
      "üìâ Step 79200: loss = 0.6162\n",
      "üìâ Step 79300: loss = 0.6513\n",
      "üìâ Step 79400: loss = 0.6507\n",
      "üìâ Step 79500: loss = 0.6638\n",
      "üìâ Step 79600: loss = 0.6333\n",
      "üìâ Step 79700: loss = 0.6696\n",
      "üìâ Step 79800: loss = 0.6654\n",
      "üìâ Step 79900: loss = 0.6635\n",
      "üìâ Step 80000: loss = 0.6533\n",
      "üìâ Step 80100: loss = 0.6373\n",
      "üìâ Step 80200: loss = 0.6567\n",
      "üìâ Step 80300: loss = 0.6481\n",
      "üìâ Step 80400: loss = 0.6458\n",
      "üìâ Step 80500: loss = 0.6154\n",
      "üìâ Step 80600: loss = 0.6351\n",
      "üìâ Step 80700: loss = 0.6576\n",
      "üìâ Step 80800: loss = 0.6436\n",
      "üìâ Step 80900: loss = 0.6496\n",
      "üìâ Step 81000: loss = 0.6470\n",
      "üìâ Step 81100: loss = 0.6572\n",
      "üìâ Step 81200: loss = 0.6589\n",
      "üìâ Step 81300: loss = 0.6404\n",
      "üìâ Step 81400: loss = 0.6561\n",
      "üìâ Step 81500: loss = 0.6460\n",
      "üìâ Step 81600: loss = 0.6474\n",
      "üìâ Step 81700: loss = 0.6477\n",
      "üìâ Step 81800: loss = 0.6340\n",
      "üìâ Step 81900: loss = 0.6350\n",
      "üìâ Step 82000: loss = 0.6321\n",
      "üìâ Step 82100: loss = 0.6534\n",
      "üìâ Step 82200: loss = 0.6413\n",
      "üìâ Step 82300: loss = 0.6448\n",
      "üìâ Step 82400: loss = 0.6548\n",
      "üìâ Step 82500: loss = 0.6547\n",
      "üìâ Step 82600: loss = 0.6403\n",
      "üìâ Step 82700: loss = 0.6337\n",
      "üìâ Step 82800: loss = 0.6417\n",
      "üìâ Step 82900: loss = 0.6556\n",
      "üìâ Step 83000: loss = 0.6744\n",
      "üìâ Step 83100: loss = 0.6612\n",
      "üìâ Step 83200: loss = 0.6446\n",
      "üìâ Step 83300: loss = 0.6393\n",
      "üìâ Step 83400: loss = 0.6098\n",
      "üìâ Step 83500: loss = 0.6472\n",
      "üìâ Step 83600: loss = 0.6422\n",
      "üìâ Step 83700: loss = 0.6773\n",
      "üìâ Step 83800: loss = 0.6349\n",
      "üìâ Step 83900: loss = 0.6428\n",
      "üìâ Step 84000: loss = 0.6382\n",
      "üìâ Step 84100: loss = 0.6396\n",
      "üìâ Step 84200: loss = 0.6261\n",
      "üìâ Step 84300: loss = 0.6625\n",
      "üìâ Step 84400: loss = 0.6389\n",
      "üìâ Step 84500: loss = 0.6514\n",
      "üìâ Step 84600: loss = 0.6381\n",
      "üìâ Step 84700: loss = 0.6338\n",
      "üìâ Step 84800: loss = 0.6314\n",
      "üìâ Step 84900: loss = 0.6334\n",
      "üìâ Step 85000: loss = 0.6576\n",
      "üìâ Step 85100: loss = 0.6528\n",
      "üìâ Step 85200: loss = 0.6364\n",
      "üìâ Step 85300: loss = 0.6646\n",
      "üìâ Step 85400: loss = 0.6350\n",
      "üìâ Step 85500: loss = 0.6301\n",
      "üìâ Step 85600: loss = 0.6426\n",
      "üìâ Step 85700: loss = 0.6278\n",
      "üìâ Step 85800: loss = 0.6561\n",
      "üìâ Step 85900: loss = 0.6300\n",
      "üìâ Step 86000: loss = 0.6329\n",
      "üìâ Step 86100: loss = 0.6434\n",
      "üìâ Step 86200: loss = 0.6288\n",
      "üìâ Step 86300: loss = 0.6446\n",
      "üìâ Step 86400: loss = 0.6463\n",
      "üìâ Step 86500: loss = 0.6146\n",
      "üìâ Step 86600: loss = 0.6654\n",
      "üìâ Step 86700: loss = 0.6259\n",
      "üìâ Step 86800: loss = 0.6601\n",
      "üìâ Step 86900: loss = 0.6502\n",
      "üìâ Step 87000: loss = 0.6382\n",
      "üìâ Step 87100: loss = 0.6360\n",
      "üìâ Step 87200: loss = 0.6593\n",
      "üìâ Step 87300: loss = 0.6377\n",
      "üìâ Step 87400: loss = 0.6371\n",
      "üìâ Step 87500: loss = 0.6381\n",
      "üìâ Step 87600: loss = 0.6450\n",
      "üìâ Step 87700: loss = 0.6544\n",
      "üìâ Step 87800: loss = 0.6280\n",
      "üìâ Step 87900: loss = 0.6391\n",
      "üìâ Step 88000: loss = 0.6375\n",
      "üìâ Step 88100: loss = 0.6384\n",
      "üìâ Step 88200: loss = 0.6548\n",
      "üìâ Step 88300: loss = 0.6513\n",
      "üìâ Step 88400: loss = 0.6471\n",
      "üìâ Step 88500: loss = 0.6411\n",
      "üìâ Step 88600: loss = 0.6502\n",
      "üìâ Step 88700: loss = 0.6469\n",
      "üìâ Step 88800: loss = 0.6471\n",
      "üìâ Step 88900: loss = 0.6376\n",
      "üìâ Step 89000: loss = 0.6450\n",
      "üìâ Step 89100: loss = 0.6508\n",
      "üìâ Step 89200: loss = 0.6298\n",
      "üìâ Step 89300: loss = 0.6343\n",
      "üìâ Step 89400: loss = 0.6367\n",
      "üìâ Step 89500: loss = 0.6295\n",
      "üìâ Step 89600: loss = 0.6405\n",
      "üìâ Step 89700: loss = 0.6396\n",
      "üìâ Step 89800: loss = 0.6265\n",
      "üìâ Step 89900: loss = 0.6565\n",
      "üìâ Step 90000: loss = 0.6535\n",
      "üìâ Step 90100: loss = 0.6370\n",
      "üìâ Step 90200: loss = 0.6492\n",
      "üìâ Step 90300: loss = 0.6363\n",
      "üìâ Step 90400: loss = 0.6332\n",
      "üìâ Step 90500: loss = 0.6452\n",
      "üìâ Step 90600: loss = 0.6351\n",
      "üìâ Step 90700: loss = 0.6487\n",
      "üìâ Step 90800: loss = 0.6220\n",
      "üìâ Step 90900: loss = 0.6408\n",
      "üìâ Step 91000: loss = 0.6429\n",
      "üìâ Step 91100: loss = 0.6405\n",
      "üìâ Step 91200: loss = 0.6403\n",
      "üìâ Step 91300: loss = 0.6364\n",
      "üìâ Step 91400: loss = 0.6431\n",
      "üìâ Step 91500: loss = 0.6378\n",
      "üìâ Step 91600: loss = 0.6433\n",
      "üìâ Step 91700: loss = 0.6499\n",
      "üìâ Step 91800: loss = 0.6416\n",
      "üìâ Step 91900: loss = 0.6569\n",
      "üìâ Step 92000: loss = 0.6552\n",
      "üìâ Step 92100: loss = 0.6315\n",
      "üìâ Step 92200: loss = 0.6538\n",
      "üìâ Step 92300: loss = 0.6390\n",
      "üìâ Step 92400: loss = 0.6290\n",
      "üìâ Step 92500: loss = 0.6380\n",
      "üìâ Step 92600: loss = 0.6501\n",
      "üìâ Step 92700: loss = 0.6461\n",
      "üìâ Step 92800: loss = 0.6545\n",
      "üìâ Step 92900: loss = 0.6382\n",
      "üìâ Step 93000: loss = 0.6450\n",
      "üìâ Step 93100: loss = 0.6308\n",
      "üìâ Step 93200: loss = 0.6428\n",
      "üìâ Step 93300: loss = 0.6417\n",
      "üìâ Step 93400: loss = 0.6379\n",
      "üìâ Step 93500: loss = 0.6324\n",
      "üìâ Step 93600: loss = 0.6612\n",
      "üìâ Step 93700: loss = 0.6181\n",
      "üìâ Step 93800: loss = 0.6569\n",
      "üìâ Step 93900: loss = 0.6283\n",
      "üìâ Step 94000: loss = 0.6398\n",
      "üìâ Step 94100: loss = 0.6324\n",
      "üìâ Step 94200: loss = 0.6462\n",
      "üìâ Step 94300: loss = 0.6475\n",
      "üìâ Step 94400: loss = 0.6489\n",
      "üìâ Step 94500: loss = 0.6446\n",
      "üìâ Step 94600: loss = 0.6373\n",
      "üìâ Step 94700: loss = 0.6363\n",
      "üìâ Step 94800: loss = 0.6529\n",
      "üìâ Step 94900: loss = 0.6450\n",
      "üìâ Step 95000: loss = 0.6241\n",
      "üìâ Step 95100: loss = 0.6424\n",
      "üìâ Step 95200: loss = 0.6526\n",
      "üìâ Step 95300: loss = 0.6352\n",
      "üìâ Step 95400: loss = 0.6401\n",
      "üìâ Step 95500: loss = 0.6415\n",
      "üìâ Step 95600: loss = 0.6306\n",
      "üìâ Step 95700: loss = 0.6530\n",
      "üìâ Step 95800: loss = 0.6556\n",
      "üìâ Step 95900: loss = 0.6333\n",
      "üìâ Step 96000: loss = 0.6361\n",
      "üìâ Step 96100: loss = 0.6341\n",
      "üìâ Step 96200: loss = 0.6390\n",
      "üìâ Step 96300: loss = 0.6464\n",
      "üìâ Step 96400: loss = 0.6256\n",
      "üìâ Step 96500: loss = 0.6206\n",
      "üìâ Step 96600: loss = 0.6262\n",
      "üìâ Step 96700: loss = 0.6269\n",
      "üìâ Step 96800: loss = 0.6464\n",
      "üìâ Step 96900: loss = 0.6294\n",
      "üìâ Step 97000: loss = 0.6330\n",
      "üìâ Step 97100: loss = 0.6269\n",
      "üìâ Step 97200: loss = 0.6362\n",
      "üìâ Step 97300: loss = 0.6327\n",
      "üìâ Step 97400: loss = 0.6306\n",
      "üìâ Step 97500: loss = 0.6416\n",
      "üìâ Step 97600: loss = 0.6497\n",
      "üìâ Step 97700: loss = 0.6422\n",
      "üìâ Step 97800: loss = 0.6355\n",
      "üìâ Step 97900: loss = 0.6496\n",
      "üìâ Step 98000: loss = 0.6437\n",
      "üìâ Step 98100: loss = 0.6422\n",
      "üìâ Step 98200: loss = 0.6276\n",
      "üìâ Step 98300: loss = 0.6601\n",
      "üìâ Step 98400: loss = 0.6472\n",
      "üìâ Step 98500: loss = 0.6346\n",
      "üìâ Step 98600: loss = 0.6425\n",
      "üìâ Step 98700: loss = 0.6370\n",
      "üìâ Step 98800: loss = 0.6378\n",
      "üìâ Step 98900: loss = 0.6494\n",
      "üìâ Step 99000: loss = 0.6151\n",
      "üìâ Step 99100: loss = 0.6319\n",
      "üìâ Step 99200: loss = 0.6402\n",
      "üìâ Step 99300: loss = 0.6532\n",
      "üìâ Step 99400: loss = 0.6497\n",
      "üìâ Step 99500: loss = 0.6390\n",
      "üìâ Step 99600: loss = 0.6422\n",
      "üìâ Step 99700: loss = 0.6293\n",
      "üìâ Step 99800: loss = 0.6526\n",
      "üìâ Step 99900: loss = 0.6573\n",
      "üìâ Step 100000: loss = 0.6542\n",
      "üìâ Step 100100: loss = 0.6495\n",
      "üìâ Step 100200: loss = 0.6479\n",
      "üìâ Step 100300: loss = 0.6462\n",
      "üìâ Step 100400: loss = 0.6391\n",
      "üìâ Step 100500: loss = 0.6274\n",
      "üìâ Step 100600: loss = 0.6340\n",
      "üìâ Step 100700: loss = 0.6399\n",
      "üìâ Step 100800: loss = 0.6386\n",
      "üìâ Step 100900: loss = 0.6065\n",
      "üìâ Step 101000: loss = 0.6330\n",
      "üìâ Step 101100: loss = 0.6249\n",
      "üìâ Step 101200: loss = 0.6252\n",
      "üìâ Step 101300: loss = 0.6269\n",
      "üìâ Step 101400: loss = 0.6005\n",
      "üìâ Step 101500: loss = 0.6414\n",
      "üìâ Step 101600: loss = 0.6171\n",
      "üìâ Step 101700: loss = 0.6160\n",
      "üìâ Step 101800: loss = 0.6170\n",
      "üìâ Step 101900: loss = 0.6318\n",
      "üìâ Step 102000: loss = 0.6218\n",
      "üìâ Step 102100: loss = 0.6367\n",
      "üìâ Step 102200: loss = 0.6184\n",
      "üìâ Step 102300: loss = 0.6332\n",
      "üìâ Step 102400: loss = 0.6440\n",
      "üìâ Step 102500: loss = 0.6132\n",
      "üìâ Step 102600: loss = 0.6353\n",
      "üìâ Step 102700: loss = 0.6275\n",
      "üìâ Step 102800: loss = 0.6375\n",
      "üìâ Step 102900: loss = 0.6158\n",
      "üìâ Step 103000: loss = 0.6375\n",
      "üìâ Step 103100: loss = 0.6315\n",
      "üìâ Step 103200: loss = 0.6147\n",
      "üìâ Step 103300: loss = 0.6421\n",
      "üìâ Step 103400: loss = 0.6248\n",
      "üìâ Step 103500: loss = 0.6317\n",
      "üìâ Step 103600: loss = 0.6189\n",
      "üìâ Step 103700: loss = 0.6258\n",
      "üìâ Step 103800: loss = 0.6245\n",
      "üìâ Step 103900: loss = 0.6404\n",
      "üìâ Step 104000: loss = 0.6349\n",
      "üìâ Step 104100: loss = 0.6248\n",
      "üìâ Step 104200: loss = 0.6015\n",
      "üìâ Step 104300: loss = 0.6050\n",
      "üìâ Step 104400: loss = 0.6173\n",
      "üìâ Step 104500: loss = 0.6190\n",
      "üìâ Step 104600: loss = 0.6313\n",
      "üìâ Step 104700: loss = 0.6346\n",
      "üìâ Step 104800: loss = 0.6233\n",
      "üìâ Step 104900: loss = 0.6291\n",
      "üìâ Step 105000: loss = 0.6244\n",
      "üìâ Step 105100: loss = 0.6265\n",
      "üìâ Step 105200: loss = 0.6408\n",
      "üìâ Step 105300: loss = 0.6120\n",
      "üìâ Step 105400: loss = 0.6267\n",
      "üìâ Step 105500: loss = 0.5997\n",
      "üìâ Step 105600: loss = 0.6428\n",
      "üìâ Step 105700: loss = 0.6226\n",
      "üìâ Step 105800: loss = 0.6138\n",
      "üìâ Step 105900: loss = 0.6261\n",
      "üìâ Step 106000: loss = 0.6144\n",
      "üìâ Step 106100: loss = 0.6145\n",
      "üìâ Step 106200: loss = 0.6244\n",
      "üìâ Step 106300: loss = 0.6088\n",
      "üìâ Step 106400: loss = 0.6174\n",
      "üìâ Step 106500: loss = 0.6358\n",
      "üìâ Step 106600: loss = 0.6310\n",
      "üìâ Step 106700: loss = 0.6358\n",
      "üìâ Step 106800: loss = 0.6343\n",
      "üìâ Step 106900: loss = 0.6497\n",
      "üìâ Step 107000: loss = 0.6439\n",
      "üìâ Step 107100: loss = 0.6140\n",
      "üìâ Step 107200: loss = 0.6268\n",
      "üìâ Step 107300: loss = 0.6092\n",
      "üìâ Step 107400: loss = 0.6324\n",
      "üìâ Step 107500: loss = 0.6292\n",
      "üìâ Step 107600: loss = 0.6416\n",
      "üìâ Step 107700: loss = 0.6226\n",
      "üìâ Step 107800: loss = 0.6212\n",
      "üìâ Step 107900: loss = 0.6137\n",
      "üìâ Step 108000: loss = 0.6100\n",
      "üìâ Step 108100: loss = 0.6199\n",
      "üìâ Step 108200: loss = 0.6248\n",
      "üìâ Step 108300: loss = 0.6217\n",
      "üìâ Step 108400: loss = 0.6409\n",
      "üìâ Step 108500: loss = 0.6304\n",
      "üìâ Step 108600: loss = 0.6446\n",
      "üìâ Step 108700: loss = 0.6171\n",
      "üìâ Step 108800: loss = 0.6286\n",
      "üìâ Step 108900: loss = 0.6394\n",
      "üìâ Step 109000: loss = 0.6184\n",
      "üìâ Step 109100: loss = 0.6392\n",
      "üìâ Step 109200: loss = 0.6364\n",
      "üìâ Step 109300: loss = 0.6237\n",
      "üìâ Step 109400: loss = 0.6256\n",
      "üìâ Step 109500: loss = 0.6197\n",
      "üìâ Step 109600: loss = 0.6282\n",
      "üìâ Step 109700: loss = 0.6367\n",
      "üìâ Step 109800: loss = 0.6264\n",
      "üìâ Step 109900: loss = 0.6228\n",
      "üìâ Step 110000: loss = 0.6365\n",
      "üìâ Step 110100: loss = 0.6251\n",
      "üìâ Step 110200: loss = 0.6146\n",
      "üìâ Step 110300: loss = 0.6230\n",
      "üìâ Step 110400: loss = 0.6219\n",
      "üìâ Step 110500: loss = 0.6530\n",
      "üìâ Step 110600: loss = 0.5924\n",
      "üìâ Step 110700: loss = 0.6439\n",
      "üìâ Step 110800: loss = 0.6416\n",
      "üìâ Step 110900: loss = 0.6319\n",
      "üìâ Step 111000: loss = 0.6179\n",
      "üìâ Step 111100: loss = 0.6126\n",
      "üìâ Step 111200: loss = 0.6518\n",
      "üìâ Step 111300: loss = 0.6258\n",
      "üìâ Step 111400: loss = 0.6478\n",
      "üìâ Step 111500: loss = 0.6202\n",
      "üìâ Step 111600: loss = 0.6239\n",
      "üìâ Step 111700: loss = 0.6360\n",
      "üìâ Step 111800: loss = 0.6298\n",
      "üìâ Step 111900: loss = 0.6214\n",
      "üìâ Step 112000: loss = 0.6241\n",
      "üìâ Step 112100: loss = 0.6105\n",
      "üìâ Step 112200: loss = 0.6375\n",
      "üìâ Step 112300: loss = 0.6061\n",
      "üìâ Step 112400: loss = 0.6230\n",
      "üìâ Step 112500: loss = 0.6339\n",
      "üìâ Step 112600: loss = 0.6339\n",
      "üìâ Step 112700: loss = 0.6243\n",
      "üìâ Step 112800: loss = 0.6380\n",
      "üìâ Step 112900: loss = 0.6371\n",
      "üìâ Step 113000: loss = 0.6342\n",
      "üìâ Step 113100: loss = 0.6466\n",
      "üìâ Step 113200: loss = 0.6467\n",
      "üìâ Step 113300: loss = 0.6136\n",
      "üìâ Step 113400: loss = 0.6159\n",
      "üìâ Step 113500: loss = 0.6174\n",
      "üìâ Step 113600: loss = 0.6475\n",
      "üìâ Step 113700: loss = 0.6318\n",
      "üìâ Step 113800: loss = 0.6292\n",
      "üìâ Step 113900: loss = 0.6207\n",
      "üìâ Step 114000: loss = 0.6127\n",
      "üìâ Step 114100: loss = 0.6267\n",
      "üìâ Step 114200: loss = 0.6319\n",
      "üìâ Step 114300: loss = 0.5887\n",
      "üìâ Step 114400: loss = 0.6147\n",
      "üìâ Step 114500: loss = 0.6195\n",
      "üìâ Step 114600: loss = 0.6058\n",
      "üìâ Step 114700: loss = 0.6312\n",
      "üìâ Step 114800: loss = 0.6278\n",
      "üìâ Step 114900: loss = 0.6124\n",
      "üìâ Step 115000: loss = 0.6439\n",
      "üìâ Step 115100: loss = 0.6239\n",
      "üìâ Step 115200: loss = 0.6395\n",
      "üìâ Step 115300: loss = 0.6277\n",
      "üìâ Step 115400: loss = 0.6151\n",
      "üìâ Step 115500: loss = 0.6287\n",
      "üìâ Step 115600: loss = 0.6433\n",
      "üìâ Step 115700: loss = 0.6322\n",
      "üìâ Step 115800: loss = 0.6341\n",
      "üìâ Step 115900: loss = 0.6306\n",
      "üìâ Step 116000: loss = 0.6288\n",
      "üìâ Step 116100: loss = 0.6468\n",
      "üìâ Step 116200: loss = 0.6385\n",
      "üìâ Step 116300: loss = 0.6289\n",
      "üìâ Step 116400: loss = 0.6128\n",
      "üìâ Step 116500: loss = 0.6277\n",
      "üìâ Step 116600: loss = 0.6395\n",
      "üìâ Step 116700: loss = 0.6126\n",
      "üìâ Step 116800: loss = 0.6503\n",
      "üìâ Step 116900: loss = 0.6373\n",
      "üìâ Step 117000: loss = 0.6318\n",
      "üìâ Step 117100: loss = 0.6362\n",
      "üìâ Step 117200: loss = 0.5966\n",
      "üìâ Step 117300: loss = 0.6242\n",
      "üìâ Step 117400: loss = 0.6266\n",
      "üìâ Step 117500: loss = 0.6356\n",
      "üìâ Step 117600: loss = 0.6216\n",
      "üìâ Step 117700: loss = 0.6310\n",
      "üìâ Step 117800: loss = 0.6298\n",
      "üìâ Step 117900: loss = 0.6317\n",
      "üìâ Step 118000: loss = 0.6202\n",
      "üìâ Step 118100: loss = 0.6328\n",
      "üìâ Step 118200: loss = 0.6229\n",
      "üìâ Step 118300: loss = 0.6139\n",
      "üìâ Step 118400: loss = 0.6336\n",
      "üìâ Step 118500: loss = 0.6353\n",
      "üìâ Step 118600: loss = 0.6481\n",
      "üìâ Step 118700: loss = 0.6286\n",
      "üìâ Step 118800: loss = 0.6243\n",
      "üìâ Step 118900: loss = 0.6264\n",
      "üìâ Step 119000: loss = 0.6102\n",
      "üìâ Step 119100: loss = 0.6315\n",
      "üìâ Step 119200: loss = 0.6228\n",
      "üìâ Step 119300: loss = 0.6235\n",
      "üìâ Step 119400: loss = 0.6229\n",
      "üìâ Step 119500: loss = 0.6271\n",
      "üìâ Step 119600: loss = 0.6200\n",
      "üìâ Step 119700: loss = 0.6276\n",
      "üìâ Step 119800: loss = 0.6226\n",
      "üìâ Step 119900: loss = 0.6158\n",
      "üìâ Step 120000: loss = 0.6319\n",
      "üìâ Step 120100: loss = 0.6309\n",
      "üìâ Step 120200: loss = 0.6321\n",
      "üìâ Step 120300: loss = 0.6366\n",
      "üìâ Step 120400: loss = 0.6325\n",
      "üìâ Step 120500: loss = 0.6299\n",
      "üìâ Step 120600: loss = 0.6151\n",
      "üìâ Step 120700: loss = 0.6320\n",
      "üìâ Step 120800: loss = 0.6180\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./codeT5-finetuned-full-javadata_small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-05T10:02:46.556031Z",
     "iopub.status.idle": "2025-05-05T10:02:46.556380Z",
     "shell.execute_reply": "2025-05-05T10:02:46.556244Z",
     "shell.execute_reply.started": "2025-05-05T10:02:46.556229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "\n",
    "# ‚úÖ 1. T·∫°o args cho predict\n",
    "eval_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./eval\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    do_predict=True,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# ‚úÖ 2. T·∫°o data_collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# ‚úÖ 3. Kh·ªüi t·∫°o trainer m·ªõi\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ‚úÖ 4. Th·ª±c hi·ªán d·ª± ƒëo√°n\n",
    "results = trainer.predict(tokenized_datasets[\"test\"])\n",
    "\n",
    "# ‚úÖ 5. Gi·∫£i m√£ v√† x·ª≠ l√Ω -100 ‚Üí 0\n",
    "decoded_preds = []\n",
    "decoded_labels = []\n",
    "\n",
    "for pred_ids, label_ids in zip(results.predictions, results.label_ids):\n",
    "    pred_ids = [int(id) if id != -100 else 0 for id in pred_ids]\n",
    "    label_ids = [int(id) if id != -100 else 0 for id in label_ids]\n",
    "\n",
    "    decoded_preds.append(tokenizer.decode(pred_ids, skip_special_tokens=True))\n",
    "    decoded_labels.append(tokenizer.decode(label_ids, skip_special_tokens=True))\n",
    "\n",
    "# ‚úÖ 6. In 5 v√≠ d·ª• ƒë·∫ßu ti√™n\n",
    "for i in range(5):\n",
    "    print(f\"\\nüîπ Sample {i}\")\n",
    "    print(\"üéØ Prediction:\", decoded_preds[i])\n",
    "    print(\"‚úÖ Reference :\", decoded_labels[i])\n",
    "\n",
    "# ‚úÖ 7. Chu·∫©n h√≥a BLEU\n",
    "references = [[ref.strip().split()] for ref in decoded_labels]\n",
    "candidates = [pred.strip().split() for pred in decoded_preds]\n",
    "\n",
    "# ‚úÖ 8. T√≠nh BLEU\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu  = corpus_bleu(references, candidates, smoothing_function=smoothie)\n",
    "bleu1 = corpus_bleu(references, candidates, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "bleu2 = corpus_bleu(references, candidates, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "bleu3 = corpus_bleu(references, candidates, weights=(1/3, 1/3, 1/3, 0), smoothing_function=smoothie)\n",
    "bleu4 = corpus_bleu(references, candidates, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "# ‚úÖ 9. Chu·∫©n b·ªã d·ªØ li·ªáu cho ROUGE (l·ªçc r·ªóng)\n",
    "filtered_hyps = []\n",
    "filtered_refs = []\n",
    "\n",
    "for hyp, ref in zip(candidates, references):\n",
    "    if len(hyp) > 0 and len(ref[0]) > 0:\n",
    "        filtered_hyps.append(\" \".join(hyp))\n",
    "        filtered_refs.append(\" \".join(ref[0]))\n",
    "\n",
    "# ‚úÖ 10. T√≠nh ROUGE\n",
    "rouge = Rouge()\n",
    "rouge_scores = rouge.get_scores(filtered_hyps, filtered_refs, avg=True)\n",
    "\n",
    "# ‚úÖ 11. T√≠nh ƒë·ªô d√†i v√† Brevity Penalty\n",
    "trans_len = sum(len(c) for c in candidates)\n",
    "ref_len = sum(len(r[0]) for r in references)\n",
    "ratio = trans_len / ref_len if ref_len > 0 else 0\n",
    "bp = 1.0 if ratio > 1.0 else np.exp(1 - 1.0 / ratio) if ratio > 0 else 0\n",
    "\n",
    "# ‚úÖ 12. In k·∫øt qu·∫£ BLEU + ROUGE\n",
    "print(\"\\nüìä Evaluation Metrics:\")\n",
    "print(f\"BLEU     : {bleu:.4f}\")\n",
    "print(f\"BLEU-1   : {bleu1:.4f}\")\n",
    "print(f\"BLEU-2   : {bleu2:.4f}\")\n",
    "print(f\"BLEU-3   : {bleu3:.4f}\")\n",
    "print(f\"BLEU-4   : {bleu4:.4f}\")\n",
    "print(f\"Brevity Penalty (bp): {bp:.4f}\")\n",
    "print(f\"Length ratio         : {ratio:.4f}\")\n",
    "print(f\"Translation length   : {trans_len}\")\n",
    "print(f\"Reference length     : {ref_len}\")\n",
    "\n",
    "print(\"\\nüî¥ ROUGE Scores:\")\n",
    "print(f\"ROUGE-1 F1        : {rouge_scores['rouge-1']['f']:.4f}\")\n",
    "print(f\"ROUGE-1 Precision : {rouge_scores['rouge-1']['p']:.4f}\")\n",
    "print(f\"ROUGE-1 Recall    : {rouge_scores['rouge-1']['r']:.4f}\")\n",
    "print(f\"ROUGE-L F1        : {rouge_scores['rouge-l']['f']:.4f}\")\n",
    "print(f\"ROUGE-L Precision : {rouge_scores['rouge-l']['p']:.4f}\")\n",
    "print(f\"ROUGE-L Recall    : {rouge_scores['rouge-l']['r']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7262129,
     "sourceId": 11582292,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 237578887,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
